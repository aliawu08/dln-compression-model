#!/usr/bin/env python3
"""DLN Core–Variable Compression Simulation (combined-paper version)

This simulation is designed to test the *updated* thesis articulated in the review note:

Core (options) compression
  Network cognition is structurally more efficient because it represents shared
  option components once (F shared factors) rather than redundantly across each
  option branch (K options). This advantage exists in stable environments and
  does not require distribution shift or stakes variation.

Variable (stakes) compression
  Stakes/exposures are structured: many options share common risk drivers.
  The environment maintains a cumulative exposure state E_t and applies a
  quadratic exposure objective. The true incremental penalty includes cross-terms
  2 E_t b_i. A network agent that (i) learns factor-level exposure structure and
  (ii) tracks E_t can "hedge once" and avoid redundant hedging. A linear agent
  that treats each option's stakes independently and ignores E_t cross-terms
  cannot exploit this structure.

Core × Variable intersection (dual-purpose)
  Dual-purpose actions are those that are good on Core (high expected reward)
  and simultaneously good on Variable (reduce marginal exposure penalty given E_t).
  Linear agents optimize Core and Variable separately, missing intersection effects.

Learning cycle (mechanism)
  Network cognition is implemented as an explicit cycle:
    1) structural hypothesis (start with factor model),
    2) frequentist predictive check (rolling-window error),
    3) branch: exploit structure if correct OR expand/update hypothesis if wrong.

We intentionally keep the environment minimal (bandit-like) so the compression
mechanism is isolated and measurable.

Explicit modeling assumptions (mirrors the paper)
-------------------------------------------------
A1. Each option i exposes an observed core-factor label c_i in {0..F-1}.
    (This is the *shared component* representation.)
A2. Each option i exposes an observed stakes loading b_i.
A3. "Prior correct" condition: rewards are generated by a factor model mu_i = m_{c_i}+eps_i.
A4. "Prior wrong" condition: rewards are generated independently per option (c_i is irrelevant).
A5. Stakes are a quadratic exposure objective in 1D state E_t. Selecting option i updates
    E_{t+1} = E_t + b_i and incurs penalty delta_P = lambda*(2*E_t*b_i + b_i^2).
    The cross-term 2*E_t*b_i creates marginal effects that per-option penalties cannot capture.
A6. Cognitive cost is a proxy (memory + compute scaling). It is not claimed as biological energy.

Outputs (paper-ready)
---------------------
- results/episode_metrics.csv        : per-seed episode metrics
- results/manifest.json              : run config + file manifest
- artifacts/tables/agg_summary.csv   : aggregated means/stds
- artifacts/figures/*.png            : figures used in the paper

Run
---
  python dln_core_variable_cycle.py --preset smoke --out ./dln_outputs
  python dln_core_variable_cycle.py --preset paper --out ./dln_outputs
"""

from __future__ import annotations

import argparse
import dataclasses
import json
import math
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Literal, Tuple

import numpy as np
import pandas as pd

Structure = Literal["structured", "unstructured", "recovery"]


def spawn_rngs(seed: int, n_agents: int = 1) -> Tuple[np.random.Generator, List[np.random.Generator]]:
    """Create independent RNG streams for environment and per-agent episode execution.

    The top-level spawn(2) is preserved from the original design so that
    env_rng is unchanged.  Agent streams are sub-spawned from the episode
    child, giving each agent a deterministic, independent stream.  Adding
    or removing agents does not shift results for existing agents (the i-th
    sub-spawn is always the same regardless of n_agents).
    """
    sequence = np.random.SeedSequence(seed)
    env_ss, episode_ss = sequence.spawn(2)
    env_rng = np.random.default_rng(env_ss)
    agent_rngs = [np.random.default_rng(s) for s in episode_ss.spawn(n_agents)]
    return env_rng, agent_rngs


# -------------------------
# Environment
# -------------------------

@dataclass(frozen=True)
class EnvSpec:
    K: int
    F: int
    T: int
    stakes: float
    reward_sigma: float
    idio_mu_sigma: float
    idio_b_sigma: float
    factor_corr: float   # correlation between factor reward m_f and factor exposure g_f
    structure: Structure
    regime_change_step: int = 0  # step at which to switch from unstructured to structured (0 = no change)
    dual_purpose: bool = True


@dataclass
class EnvInstance:
    spec: EnvSpec
    c: np.ndarray        # (K,) core factor label
    mu: np.ndarray       # (K,) reward mean (active; may be swapped at regime change)
    b: np.ndarray        # (K,) exposure loading
    dual_mask: np.ndarray  # (K,) dual-purpose flag (by construction)
    mu_alt: np.ndarray | None = None  # (K,) post-regime-change reward mean (recovery only)


def make_env(rng: np.random.Generator, spec: EnvSpec) -> EnvInstance:
    K, F = spec.K, spec.F

    # Factor assignment (observable shared component)
    c = rng.integers(0, F, size=K)

    # Factor-level Core and Variable parameters
    m = rng.normal(0.0, 1.0, size=F)
    m_std = (m - m.mean()) / (m.std() + 1e-9)

    z = rng.normal(0.0, 1.0, size=F)
    g = spec.factor_corr * m_std + math.sqrt(max(0.0, 1.0 - spec.factor_corr**2)) * z
    g = (g - g.mean()) / (g.std() + 1e-9)

    # Rewards
    mu_alt = None
    if spec.structure == "structured":
        mu = m[c] + rng.normal(0.0, spec.idio_mu_sigma, size=K)
    elif spec.structure == "recovery":
        # Start unstructured, switch to structured at regime_change_step
        mu = rng.normal(0.0, 1.0, size=K)
        mu = (mu - mu.mean()) / (mu.std() + 1e-9)
        mu_struct = m[c] + rng.normal(0.0, spec.idio_mu_sigma, size=K)
        mu_alt = (mu_struct - mu_struct.mean()) / (mu_struct.std() + 1e-9)
    else:
        # Prior-wrong for the factor hypothesis: independent option means
        mu = rng.normal(0.0, 1.0, size=K)

    mu = (mu - mu.mean()) / (mu.std() + 1e-9)

    # Stakes loadings with factor structure (Variable compression target)
    b = g[c] + rng.normal(0.0, spec.idio_b_sigma, size=K)
    b = (b - b.mean()) / (b.std() + 1e-9)

    # Dual-purpose construction: ensure at least one high-mu option has exposure opposite
    dual_mask = np.zeros(K, dtype=bool)
    if spec.dual_purpose and spec.stakes > 0:
        top = np.argsort(mu)[-max(5, K // 10):]
        # prefer a dual-purpose option in the top set: high mu and negative exposure
        cand = [i for i in top if b[i] < 0]
        if not cand:
            i0 = int(top[-1])
            b[i0] = -abs(b[i0])  # enforce hedge direction
            cand = [i0]
        dual_mask[cand] = True

    return EnvInstance(spec=spec, c=c, mu=mu, b=b, dual_mask=dual_mask, mu_alt=mu_alt)


def step_reward(rng: np.random.Generator, mu_i: float, sigma: float) -> float:
    return float(mu_i + rng.normal(0.0, sigma))


def step_penalty(E: float, b_i: float, stakes: float) -> float:
    # Quadratic exposure objective: stakes*((E+b)^2 - E^2) = stakes*(2Eb + b^2)
    return float(stakes * ((E + b_i) ** 2 - E ** 2))


# -------------------------
# Cognitive cost proxy
# -------------------------

@dataclass(frozen=True)
class CogCostWeights:
    mem: float = 0.01
    compute: float = 0.001
    switch: float = 25.0  # overhead when expanding to tabular


@dataclass
class CogCostCounter:
    mem_units: float = 0.0
    compute_units: float = 0.0
    switches: int = 0

    def total(self, w: CogCostWeights) -> float:
        return w.mem * self.mem_units + w.compute * self.compute_units + w.switch * self.switches


# -------------------------
# Contraction (return transition)
# -------------------------

@dataclass(frozen=True)
class ContractionConfig:
    """Configuration for the contraction (return) transition.

    Parameters
    ----------
    enabled:
        Whether contraction logic is active at all.
    holdout_steps (n_contract):
        Minimum number of steps to wait in the expanded (tabular) model before
        opening a contraction evaluation window.
    window (w):
        Length of the evaluation window in which the agent runs a shadow factor
        model and compares predictive error to the tabular model.
    margin (theta_contract):
        Required relative improvement (fraction) for contracting.
        Contract if MSE_factor < (1 - margin) * MSE_tab.
    init_from_tabular:
        Whether to initialize the shadow factor model from current tabular
        estimates (group means of q-values).
    free_tabular_on_contract:
        Whether to drop tabular state on successful contraction, returning memory
        to O(F) scaling (the compressed representation).
    """

    enabled: bool = True
    holdout_steps: int = 50
    window: int = 20
    margin: float = 0.10
    init_from_tabular: bool = True
    free_tabular_on_contract: bool = True
    eval_explore_eps: float = 0.0  # forced exploration rate during eval windows (0 = use default)
    eval_warmup: int = 0  # steps at start of eval window to skip for error comparison (shadow still learns)


@dataclass
class ContractionState:
    """Mutable state for contraction evaluation windows."""
    steps_since_last_eval: int = 0
    in_eval_window: bool = False
    eval_steps_remaining: int = 0

    shadow_m: np.ndarray | None = None
    shadow_n: np.ndarray | None = None

    err_tab: List[float] = field(default_factory=list)
    err_shadow: List[float] = field(default_factory=list)

    def start(self, window: int) -> None:
        self.in_eval_window = True
        self.eval_steps_remaining = window
        self.err_tab.clear()
        self.err_shadow.clear()

    def stop(self) -> None:
        self.in_eval_window = False
        self.eval_steps_remaining = 0
        self.shadow_m = None
        self.shadow_n = None
        self.err_tab.clear()
        self.err_shadow.clear()


# -------------------------
# Agents
# -------------------------

class Agent:
    name: str

    def reset(self, env: EnvInstance, rng: np.random.Generator) -> None:
        raise NotImplementedError

    def act(self, t: int, env: EnvInstance) -> int:
        raise NotImplementedError

    def observe(self, a: int, r: float, pen: float, env: EnvInstance) -> None:
        raise NotImplementedError

    def cog_cost(self) -> float:
        raise NotImplementedError


class DotRandom(Agent):
    name = "Dot"

    def __init__(self, cost_w: CogCostWeights) -> None:
        self.cost_w = cost_w
        self.rng: np.random.Generator | None = None
        self.K = 0
        self.cost = CogCostCounter()

    def reset(self, env: EnvInstance, rng: np.random.Generator) -> None:
        self.rng = rng
        self.K = env.spec.K
        self.cost = CogCostCounter(mem_units=1.0, compute_units=0.0, switches=0)

    def act(self, t: int, env: EnvInstance) -> int:
        assert self.rng is not None
        self.cost.compute_units += 1.0
        return int(self.rng.integers(0, self.K))

    def observe(self, a: int, r: float, pen: float, env: EnvInstance) -> None:
        return

    def cog_cost(self) -> float:
        return self.cost.total(self.cost_w)


class LinearTabular(Agent):
    """Linear regime: item-by-item frequentist means; ignores E cross-term."""

    name = "Linear"

    def __init__(self, eps0: float, cost_w: CogCostWeights) -> None:
        self.eps0 = eps0
        self.cost_w = cost_w
        self.rng: np.random.Generator | None = None
        self.K = 0
        self.q: np.ndarray | None = None
        self.n: np.ndarray | None = None
        self.cost = CogCostCounter()

    def reset(self, env: EnvInstance, rng: np.random.Generator) -> None:
        self.rng = rng
        self.K = env.spec.K
        self.q = np.zeros(self.K, dtype=float)
        self.n = np.zeros(self.K, dtype=float)
        # memory scales with K
        self.cost = CogCostCounter(mem_units=2.0 * self.K, compute_units=0.0, switches=0)

    def _eps(self, t: int) -> float:
        return min(1.0, self.eps0 / math.sqrt(max(1, t)))

    def act(self, t: int, env: EnvInstance) -> int:
        assert self.rng is not None and self.q is not None and self.n is not None
        if self.rng.random() < self._eps(t):
            self.cost.compute_units += 1.0
            return int(self.rng.integers(0, self.K))

        # Variable heuristic ignores E cross-term: uses only local b^2
        score = self.q - env.spec.stakes * (env.b ** 2)
        self.cost.compute_units += float(self.K)
        return int(np.argmax(score))

    def observe(self, a: int, r: float, pen: float, env: EnvInstance) -> None:
        assert self.q is not None and self.n is not None
        self.n[a] += 1.0
        self.q[a] += (r - self.q[a]) / self.n[a]
        self.cost.compute_units += 5.0

    def cog_cost(self) -> float:
        return self.cost.total(self.cost_w)


class ThompsonFactor(Agent):
    """Thompson Sampling with known factor structure (Bayesian baseline).

    Same representational structure as Network (factor-level, O(F) memory):
      - Knows factor labels c_i (shared component representation)
      - Maintains per-factor Normal-Normal conjugate posterior
      - Action selection via posterior sampling (Thompson sampling)

    Key differences from NetworkCycle:
      - Bayesian (posterior sampling) vs frequentist (eps-greedy + rolling MSE)
      - No predictive test, no expansion/contraction (stays in factor mode always)
      - No exposure tracking (ignores E_t cross-terms, uses local b² like Linear)
      - Exploration via posterior uncertainty, not eps-greedy

    This agent isolates whether the DLN learning cycle adds value beyond
    standard Bayesian factor-aware methods:
      - If Thompson ≈ Network on C1 (core compression): cycle adds nothing in stable envs
      - If Network >> Thompson on C2 (stakes): exposure tracking is the key mechanism
      - If Network >> Thompson under regime change: revision cycle matters for recovery
    """

    name = "Thompson-Factor"

    def __init__(
        self,
        cost_w: CogCostWeights,
        sigma_prior: float = 1.0,
        sigma_r: float = 0.25,
        local_L: int = 5,
    ) -> None:
        self.cost_w = cost_w
        self.sigma_prior = sigma_prior
        self.sigma_r = sigma_r
        self.local_L = local_L

        self.rng: np.random.Generator | None = None
        self.K = 0
        self.F = 0

        # Per-factor posterior: N(mu_post[f], var_post[f])
        self.mu_post: np.ndarray | None = None
        self.var_post: np.ndarray | None = None
        self.n_f: np.ndarray | None = None

        self.cost = CogCostCounter()

        # Metrics (compatible with NetworkCycle interface)
        self.expansions: int = 0
        self.contractions: int = 0
        self.revisions: int = 0
        self.contract_evals: int = 0
        self.steps_factor: int = 0
        self.steps_tabular: int = 0

    def reset(self, env: EnvInstance, rng: np.random.Generator) -> None:
        self.rng = rng
        self.K = env.spec.K
        self.F = env.spec.F

        # Prior: m_f ~ N(0, sigma_prior²) for each factor
        self.mu_post = np.zeros(self.F, dtype=float)
        self.var_post = np.full(self.F, self.sigma_prior ** 2, dtype=float)
        self.n_f = np.zeros(self.F, dtype=float)

        self.expansions = 0
        self.contractions = 0
        self.revisions = 0
        self.contract_evals = 0
        self.steps_factor = 0
        self.steps_tabular = 0

        # Memory scales with F (same as Network in factor mode)
        self.cost = CogCostCounter(mem_units=3.0 * self.F, compute_units=0.0, switches=0)

    def act(self, t: int, env: EnvInstance) -> int:
        assert self.rng is not None and self.mu_post is not None and self.var_post is not None

        stakes = env.spec.stakes

        # Thompson sampling: draw from posterior for each factor
        samples = self.rng.normal(self.mu_post, np.sqrt(self.var_post))
        self.cost.compute_units += float(self.F)

        # Select best factor by sampled value (no exposure adjustment at factor level)
        best_f = int(np.argmax(samples))

        # Local search within factor (same L-bounded search as Network)
        idx = np.where(env.c == best_f)[0]
        if idx.size == 0:
            self.cost.compute_units += 1.0
            return int(self.rng.integers(0, self.K))

        L = min(self.local_L, idx.size)
        cand = self.rng.choice(idx, size=L, replace=False)

        # Score candidates using sampled factor value and local b² penalty
        # (no E_t cross-term — Thompson doesn't track cumulative exposure)
        pen_local = stakes * (env.b[cand] ** 2)
        util_pred = samples[best_f] - pen_local
        self.cost.compute_units += float(L)
        return int(cand[int(np.argmax(util_pred))])

    def observe(self, a: int, r: float, pen: float, env: EnvInstance) -> None:
        assert self.mu_post is not None and self.var_post is not None and self.n_f is not None

        self.steps_factor += 1

        f = int(env.c[a])
        self.n_f[f] += 1.0

        # Normal-Normal conjugate update:
        #   prior:      m_f ~ N(mu_post, var_post)
        #   likelihood: r | m_f ~ N(m_f, sigma_r²)
        #   posterior:  N(mu_new, var_new)
        sigma_r_sq = self.sigma_r ** 2
        var_new = 1.0 / (1.0 / self.var_post[f] + 1.0 / sigma_r_sq)
        mu_new = var_new * (self.mu_post[f] / self.var_post[f] + r / sigma_r_sq)
        self.mu_post[f] = mu_new
        self.var_post[f] = var_new

        self.cost.compute_units += 5.0

    def cog_cost(self) -> float:
        return self.cost.total(self.cost_w)


class UCB1Agent(Agent):
    """UCB1 agent: Linear stage with confidence-bound action selection.

    DLN Stage: Linear (null belief graph, K independent estimates).

    Same representational topology as LinearTabular: per-option sample means
    and visit counts, no cross-option information sharing.  Differs only in
    action selection: UCB confidence bounds instead of epsilon-greedy.

    Key properties:
      - Memory: O(K) — identical to Linear
      - Compute: O(K) per step — scan all options for UCB selection
      - Stakes handling: local b_i^2 heuristic, no E_t tracking
      - No factor structure, no expansion/contraction, no structural learning
    """

    name = "UCB1"

    def __init__(self, cost_w: CogCostWeights, c: float = math.sqrt(2)) -> None:
        self.cost_w = cost_w
        self.c = c
        self.rng: np.random.Generator | None = None
        self.K = 0
        self.mu_hat: np.ndarray | None = None
        self.n: np.ndarray | None = None
        self.cost = CogCostCounter()

        # Metrics (compatible with NetworkCycle interface)
        self.expansions: int = 0
        self.contractions: int = 0
        self.revisions: int = 0
        self.contract_evals: int = 0
        self.steps_factor: int = 0
        self.steps_tabular: int = 0

    def reset(self, env: EnvInstance, rng: np.random.Generator) -> None:
        self.rng = rng
        self.K = env.spec.K
        self.mu_hat = np.zeros(self.K, dtype=float)
        self.n = np.zeros(self.K, dtype=float)
        # Memory scales with K — same as Linear
        self.cost = CogCostCounter(mem_units=2.0 * self.K, compute_units=0.0, switches=0)
        self.expansions = 0
        self.contractions = 0
        self.revisions = 0
        self.contract_evals = 0
        self.steps_factor = 0
        self.steps_tabular = 0

    def act(self, t: int, env: EnvInstance) -> int:
        assert self.rng is not None and self.mu_hat is not None and self.n is not None
        stakes = env.spec.stakes

        # Prefer unvisited arms (UCB = infinity for unvisited)
        unvisited = np.where(self.n == 0)[0]
        if len(unvisited) > 0:
            self.cost.compute_units += 1.0
            return int(self.rng.choice(unvisited))

        # UCB1: mu_hat + c * sqrt(ln(t) / n_i)
        ucb_values = self.mu_hat + self.c * np.sqrt(np.log(t) / self.n)
        # Stakes heuristic: same local b^2 as Linear, no E_t tracking
        score = ucb_values - stakes * (env.b ** 2)
        self.cost.compute_units += float(self.K)
        return int(np.argmax(score))

    def observe(self, a: int, r: float, pen: float, env: EnvInstance) -> None:
        assert self.mu_hat is not None and self.n is not None
        self.n[a] += 1.0
        self.mu_hat[a] += (r - self.mu_hat[a]) / self.n[a]
        self.cost.compute_units += 5.0

    def cog_cost(self) -> float:
        return self.cost.total(self.cost_w)


class HierBayes(Agent):
    """Hierarchical Bayesian agent with factor-level shrinkage.

    DLN Stage: Network (standard, without DLN-specific mechanisms).

    Implements a Normal-Normal hierarchical model with conjugate updates:
      Group level:  m_f ~ N(mu_0, sigma_0^2) for each factor f
      Option level: mu_i | m_{c_i} ~ N(m_{c_i}, tau^2)
      Observations: r ~ N(mu_{a_t}, sigma_r^2)

    Key properties:
      - Receives factor labels (A1), same as Thompson-Factor and Network
      - Performs hierarchical shrinkage: evidence from one option updates
        the group-level posterior, which propagates to all factor-mates
      - Thompson sampling for action selection with shrinkage-adjusted posteriors
      - Memory: O(F + K) — both group-level and option-level posteriors
      - Stakes handling: local b_i^2 heuristic, NO cumulative exposure tracking
      - No predictive test, no expansion/contraction, no structural revision
    """

    name = "HierBayes"

    def __init__(
        self,
        cost_w: CogCostWeights,
        mu_0: float = 0.0,
        sigma_0: float = 10.0,
        tau: float = 1.0,
        sigma_r: float = 0.25,
        local_L: int = 5,
    ) -> None:
        self.cost_w = cost_w
        self.mu_0 = mu_0
        self.sigma_0 = sigma_0
        self.tau = tau
        self.sigma_r = sigma_r
        self.local_L = local_L

        self.rng: np.random.Generator | None = None
        self.K = 0
        self.F = 0

        # Group-level posteriors: N(group_mu[f], group_var[f])
        self.group_mu: np.ndarray | None = None
        self.group_var: np.ndarray | None = None

        # Option-level sufficient statistics
        self.opt_sum: np.ndarray | None = None  # sum of rewards for each option
        self.opt_n: np.ndarray | None = None    # count of observations

        self.cost = CogCostCounter()

        # Metrics (compatible with NetworkCycle interface)
        self.expansions: int = 0
        self.contractions: int = 0
        self.revisions: int = 0
        self.contract_evals: int = 0
        self.steps_factor: int = 0
        self.steps_tabular: int = 0

    def reset(self, env: EnvInstance, rng: np.random.Generator) -> None:
        self.rng = rng
        self.K = env.spec.K
        self.F = env.spec.F

        # Group-level prior: m_f ~ N(mu_0, sigma_0^2)
        self.group_mu = np.full(self.F, self.mu_0, dtype=float)
        self.group_var = np.full(self.F, self.sigma_0 ** 2, dtype=float)

        # Option-level: just track sufficient statistics
        self.opt_sum = np.zeros(self.K, dtype=float)
        self.opt_n = np.zeros(self.K, dtype=float)

        # Memory: F group-level params + K option-level params
        self.cost = CogCostCounter(
            mem_units=3.0 * self.F + 2.0 * self.K,
            compute_units=0.0,
            switches=0,
        )

        self.expansions = 0
        self.contractions = 0
        self.revisions = 0
        self.contract_evals = 0
        self.steps_factor = 0
        self.steps_tabular = 0

    def _shrinkage_posterior(self, i: int, env: EnvInstance) -> Tuple[float, float]:
        """Compute shrinkage-adjusted posterior mean and variance for option i.

        The posterior for mu_i is a precision-weighted combination of:
          - the option-level data (sample mean with precision n_i / sigma_r^2)
          - the group-level prior (group_mu with precision 1 / (group_var + tau^2))
        """
        f = int(env.c[i])
        sigma_r_sq = self.sigma_r ** 2
        tau_sq = self.tau ** 2

        # Group-level prior for this option
        group_prec = 1.0 / (self.group_var[f] + tau_sq)
        group_mean = self.group_mu[f]

        if self.opt_n[i] == 0:
            return group_mean, 1.0 / group_prec

        # Option-level data precision
        opt_prec = self.opt_n[i] / sigma_r_sq
        opt_mean = self.opt_sum[i] / self.opt_n[i]

        # Shrinkage posterior
        total_prec = opt_prec + group_prec
        post_mean = (opt_prec * opt_mean + group_prec * group_mean) / total_prec
        post_var = 1.0 / total_prec

        return post_mean, post_var

    def act(self, t: int, env: EnvInstance) -> int:
        assert self.rng is not None and self.group_mu is not None and self.group_var is not None

        stakes = env.spec.stakes

        # Thompson sampling: draw from group posteriors to select best factor
        group_samples = self.rng.normal(self.group_mu, np.sqrt(np.maximum(self.group_var, 1e-12)))
        self.cost.compute_units += float(self.F)
        best_f = int(np.argmax(group_samples))

        # Local search within factor (same L-bounded search as Network)
        idx = np.where(env.c == best_f)[0]
        if idx.size == 0:
            self.cost.compute_units += 1.0
            return int(self.rng.integers(0, self.K))

        L = min(self.local_L, idx.size)
        cand = self.rng.choice(idx, size=L, replace=False)

        # Sample from shrinkage-adjusted posteriors for candidates
        samples = np.empty(L, dtype=float)
        for j, ci in enumerate(cand):
            mu_s, var_s = self._shrinkage_posterior(ci, env)
            samples[j] = self.rng.normal(mu_s, math.sqrt(max(var_s, 1e-12)))

        # Stakes heuristic: local b^2 only, no E_t tracking
        pen_local = stakes * (env.b[cand] ** 2)
        util_pred = samples - pen_local
        self.cost.compute_units += float(L)
        return int(cand[int(np.argmax(util_pred))])

    def observe(self, a: int, r: float, pen: float, env: EnvInstance) -> None:
        assert (self.group_mu is not None and self.group_var is not None
                and self.opt_sum is not None and self.opt_n is not None)

        self.steps_factor += 1
        f = int(env.c[a])
        sigma_r_sq = self.sigma_r ** 2
        tau_sq = self.tau ** 2

        # Update option-level sufficient statistics
        self.opt_n[a] += 1.0
        self.opt_sum[a] += r

        # Update group-level posterior for factor f
        # Group posterior combines the prior with evidence from all options in factor f
        prior_prec = 1.0 / (self.sigma_0 ** 2)
        total_prec = prior_prec
        weighted_sum = prior_prec * self.mu_0

        idx_f = np.where(env.c == f)[0]
        for i in idx_f:
            if self.opt_n[i] > 0:
                # Each option contributes: its sample mean with
                # precision = 1 / (sigma_r^2 / n_i + tau^2)
                opt_mean = self.opt_sum[i] / self.opt_n[i]
                opt_obs_var = sigma_r_sq / self.opt_n[i] + tau_sq
                opt_prec = 1.0 / opt_obs_var
                total_prec += opt_prec
                weighted_sum += opt_prec * opt_mean

        self.group_var[f] = 1.0 / total_prec
        self.group_mu[f] = weighted_sum / total_prec

        self.cost.compute_units += 10.0

    def cog_cost(self) -> float:
        return self.cost.total(self.cost_w)


class RWFeatureAgent(Agent):
    """Rescorla-Wagner Feature agent: Linear-Plus stage with associative learning.

    DLN Stage: Linear-Plus (designer-specified cross-option structure).

    Implements feature-based associative learning using the Rescorla-Wagner
    delta rule (Rescorla & Wagner, 1972). Each option i is represented by a
    one-hot feature vector over its factor label c_i. The agent maintains F
    feature weights updated by error-driven learning. When option i is
    observed, the weight for factor c_i updates, changing the predicted
    value for ALL options sharing that factor.

    Cognitive science pedigree: The Rescorla-Wagner learning rule is one of
    the most replicated models in behavioral science, validated across species,
    tasks, and decades. Feature-based generalization using associative weights
    is the foundation of the RW tradition and its descendants (Pearce-Hall,
    Mackintosh, etc.).

    Key properties:
      - Weight vector w of dimension F (one weight per factor)
      - Predicted value: V_i = w[c_i] (factor-level lookup)
      - Update: w[c_i] += alpha * (r - V_i)  (delta rule)
      - Memory: O(F) — same as Thompson-Factor
      - Action selection: epsilon-greedy (matches existing Linear agent)
      - Stakes handling: local b_i^2 heuristic, no E_t tracking
      - No structural learning, cannot discover or revise features
    """

    name = "RW-Feature"

    def __init__(
        self,
        eps0: float,
        cost_w: CogCostWeights,
        alpha: float = 0.1,
        local_L: int = 5,
    ) -> None:
        self.eps0 = eps0
        self.cost_w = cost_w
        self.alpha = alpha  # RW learning rate
        self.local_L = local_L

        self.rng: np.random.Generator | None = None
        self.K = 0
        self.F = 0

        # Feature weights: one per factor
        self.w: np.ndarray | None = None  # (F,)

        self.cost = CogCostCounter()

        # Metrics (compatible with NetworkCycle interface)
        self.expansions: int = 0
        self.contractions: int = 0
        self.revisions: int = 0
        self.contract_evals: int = 0
        self.steps_factor: int = 0
        self.steps_tabular: int = 0

    def reset(self, env: EnvInstance, rng: np.random.Generator) -> None:
        self.rng = rng
        self.K = env.spec.K
        self.F = env.spec.F

        # Initialize weights to zero (standard in RW literature)
        self.w = np.zeros(self.F, dtype=float)

        # Memory: F weights — same scaling as Thompson-Factor
        self.cost = CogCostCounter(mem_units=float(self.F), compute_units=0.0, switches=0)

        self.expansions = 0
        self.contractions = 0
        self.revisions = 0
        self.contract_evals = 0
        self.steps_factor = 0
        self.steps_tabular = 0

    def _eps(self, t: int) -> float:
        return min(1.0, self.eps0 / math.sqrt(max(1, t)))

    def act(self, t: int, env: EnvInstance) -> int:
        assert self.rng is not None and self.w is not None
        stakes = env.spec.stakes

        if self.rng.random() < self._eps(t):
            self.cost.compute_units += 1.0
            return int(self.rng.integers(0, self.K))

        # Predicted value for each option: V_i = w[c_i]
        V = self.w[env.c]  # (K,) — vectorized factor-level lookup
        self.cost.compute_units += float(self.F)

        # Pick best factor by predicted value
        best_f = int(np.argmax(self.w))

        # Local search within factor (same L-bounded search as Thompson-Factor)
        idx = np.where(env.c == best_f)[0]
        if idx.size == 0:
            self.cost.compute_units += 1.0
            return int(self.rng.integers(0, self.K))

        L = min(self.local_L, idx.size)
        cand = self.rng.choice(idx, size=L, replace=False)

        # Score: predicted value - local stakes penalty (no E_t)
        scores = V[cand] - stakes * (env.b[cand] ** 2)
        self.cost.compute_units += float(L)
        return int(cand[int(np.argmax(scores))])

    def observe(self, a: int, r: float, pen: float, env: EnvInstance) -> None:
        assert self.w is not None

        self.steps_factor += 1
        f = int(env.c[a])

        # Rescorla-Wagner delta rule: w[f] += alpha * (r - w[f])
        prediction_error = r - self.w[f]
        self.w[f] += self.alpha * prediction_error

        self.cost.compute_units += 5.0

    def cog_cost(self) -> float:
        return self.cost.total(self.cost_w)


class LatentFactorEM(Agent):
    """Latent Factor EM agent: Network stage with learned structure.

    DLN Stage: Network (standard, without DLN-specific mechanisms).

    Implements online Expectation-Maximization for a Gaussian mixture model
    over option rewards. Unlike HierBayes, this agent does NOT receive factor
    labels (A1). It must discover which options belong to which factor by
    clustering observed rewards. It knows F (number of factors) but not
    the assignments.

    Key properties:
      - Soft assignment matrix gamma[i][f] = P(z_i = f | data), size K × F
      - Factor means m_f, size F
      - Per-option observation history (mu_hat_i, n_i)
      - Action selection: V_i = sum_f gamma[i][f] * m_f + Thompson noise
      - Memory: O(K*F) — higher than HierBayes's O(F+K), cost of learning structure
      - Stakes handling: local b_i^2 heuristic, no E_t tracking
      - No expansion/contraction, no predictive test, no E_t
    """

    name = "LatentFactorEM"

    def __init__(
        self,
        cost_w: CogCostWeights,
        sigma_r: float = 0.25,
        local_L: int = 5,
    ) -> None:
        self.cost_w = cost_w
        self.sigma_r = sigma_r
        self.local_L = local_L

        self.rng: np.random.Generator | None = None
        self.K = 0
        self.F = 0

        # Soft assignment matrix
        self.gamma: np.ndarray | None = None  # (K, F)

        # Factor means
        self.m_f: np.ndarray | None = None  # (F,)

        # Per-option observation history
        self.opt_mu: np.ndarray | None = None  # (K,)
        self.opt_n: np.ndarray | None = None   # (K,)

        self.cost = CogCostCounter()

        # Metrics (compatible with NetworkCycle interface)
        self.expansions: int = 0
        self.contractions: int = 0
        self.revisions: int = 0
        self.contract_evals: int = 0
        self.steps_factor: int = 0
        self.steps_tabular: int = 0

    def reset(self, env: EnvInstance, rng: np.random.Generator) -> None:
        self.rng = rng
        self.K = env.spec.K
        self.F = env.spec.F

        # Uniform initial assignments
        self.gamma = np.full((self.K, self.F), 1.0 / self.F, dtype=float)

        # Initialize factor means spread across reward range
        self.m_f = rng.normal(0.0, 0.5, size=self.F)

        # Per-option stats
        self.opt_mu = np.zeros(self.K, dtype=float)
        self.opt_n = np.zeros(self.K, dtype=float)

        # Memory: K*F (assignment matrix) + F (factor means) + 2K (per-option stats)
        self.cost = CogCostCounter(
            mem_units=float(self.K * self.F + self.F + 2 * self.K),
            compute_units=0.0,
            switches=0,
        )

        self.expansions = 0
        self.contractions = 0
        self.revisions = 0
        self.contract_evals = 0
        self.steps_factor = 0
        self.steps_tabular = 0

    def act(self, t: int, env: EnvInstance) -> int:
        assert (self.rng is not None and self.gamma is not None
                and self.m_f is not None)

        stakes = env.spec.stakes

        # Pick best factor by mean
        best_f = int(np.argmax(self.m_f))

        # Local search: find options most likely to belong to best factor
        factor_probs = self.gamma[:, best_f]
        n_cand = min(self.local_L, self.K)
        top_idx = np.argsort(factor_probs)[-n_cand:]

        # Compute expected value for candidates: V_i = sum_f gamma[i][f] * m_f
        V = self.gamma[top_idx] @ self.m_f

        # Thompson-style exploration noise (decreasing with observations)
        sigma_explore = 1.0 / math.sqrt(max(1, t))
        noise = self.rng.normal(0.0, sigma_explore, size=n_cand)

        # Stakes heuristic: local b^2, no E_t
        scores = V + noise - stakes * (env.b[top_idx] ** 2)

        self.cost.compute_units += float(self.K + self.F)
        return int(top_idx[int(np.argmax(scores))])

    def observe(self, a: int, r: float, pen: float, env: EnvInstance) -> None:
        assert (self.gamma is not None and self.m_f is not None
                and self.opt_mu is not None and self.opt_n is not None)

        self.steps_factor += 1
        sigma_r_sq = self.sigma_r ** 2

        # Update per-option stats
        self.opt_n[a] += 1.0
        self.opt_mu[a] += (r - self.opt_mu[a]) / self.opt_n[a]

        # E-step: update assignments for option a using log-space for stability
        log_gamma = np.empty(self.F, dtype=float)
        for f in range(self.F):
            log_prior = math.log(max(self.gamma[a, f], 1e-300))
            log_lik = -0.5 * (r - self.m_f[f]) ** 2 / sigma_r_sq
            log_gamma[f] = log_prior + log_lik

        # Normalize in log-space (log-sum-exp trick)
        log_max = np.max(log_gamma)
        log_gamma -= log_max
        gamma_new = np.exp(log_gamma)
        gamma_sum = gamma_new.sum()
        if gamma_sum > 0:
            self.gamma[a] = gamma_new / gamma_sum
        else:
            self.gamma[a] = 1.0 / self.F

        # M-step: update factor means using weighted option means
        for f in range(self.F):
            weighted_sum = 0.0
            weight_total = 0.0
            for i in range(self.K):
                if self.opt_n[i] > 0:
                    w = self.gamma[i, f] * self.opt_n[i]
                    weighted_sum += w * self.opt_mu[i]
                    weight_total += w
            if weight_total > 0:
                self.m_f[f] = weighted_sum / weight_total

        self.cost.compute_units += float(self.K * self.F)

    def cog_cost(self) -> float:
        return self.cost.total(self.cost_w)


class NetworkCycle(Agent):
    """Network regime with an explicit structural learning cycle.

    Hypotheses / representations (Level 1):
      - factor model G_F: rewards depend on factor f=c_i (compressed: O(F))
      - tabular model G_tab: rewards depend on option i (expanded: O(K))

    Variable block:
      - learns factor-level exposure b_f (mean b within factor) online
      - tracks cumulative exposure E_t and uses the marginal penalty 2 E_t b_i

    Revision cycle (Level 2):
      1) start in the factor model
      2) predictive test: rolling-window factor prediction error
      3) on mismatch: expand to tabular (G_F -> G_tab)
      4) contraction: while in tabular, periodically open an evaluation window,
         initialize a shadow factor model from tabular estimates, and contract
         back (G_tab -> G_F) if predictive error improves by a margin.

    Ablations:
      - Network-NoTest: disables the predictive test (never expands)
      - Network-NoUpdate: runs the test but disallows transitions
      - Network-NoContract: allows expansion but disables the return transition
    """

    def __init__(
        self,
        eps0: float,
        window: int,
        margin: float,
        allow_test: bool,
        allow_update: bool,
        cost_w: CogCostWeights,
        local_L: int = 5,
        *,
        allow_contract: bool = True,
        expand_mse_threshold: float = 1.5,
        contraction: ContractionConfig | None = None,
    ) -> None:
        self.eps0 = eps0
        self.window = window
        self.margin = margin
        self.allow_test = allow_test
        self.allow_update = allow_update
        self.allow_contract = allow_contract
        self.expand_mse_threshold = expand_mse_threshold
        self.cost_w = cost_w
        self.local_L = local_L

        if self.allow_test and self.allow_update and self.allow_contract:
            self.name = "Network-Full"
        elif self.allow_test and self.allow_update and (not self.allow_contract):
            self.name = "Network-NoContract"
        elif (not self.allow_test) and self.allow_update:
            self.name = "Network-NoTest"
        elif self.allow_test and (not self.allow_update):
            self.name = "Network-NoUpdate"
        else:
            self.name = "Network-NoTestNoUpdate"

        # contraction config/state
        if contraction is None:
            contraction = ContractionConfig(
                enabled=True,
                holdout_steps=max(50, 3 * window),
                window=window,
                margin=margin,
                init_from_tabular=True,
                free_tabular_on_contract=True,
            )
        self.contraction_cfg = contraction
        self.contraction_state = ContractionState()

        self.rng: np.random.Generator | None = None
        self.K = 0
        self.F = 0

        # factor reward and exposure estimates
        self.m_hat: np.ndarray | None = None
        self.n_f: np.ndarray | None = None
        self.b_hat: np.ndarray | None = None
        self.n_b: np.ndarray | None = None

        # tabular reward estimates (allocated lazily)
        self.q: np.ndarray | None = None
        self.n: np.ndarray | None = None
        self.tabular_active: bool = False

        self.E: float = 0.0
        self.model: Literal["factor", "tabular"] = "factor"

        # metrics
        self.expansions: int = 0      # allocating tabular state
        self.contractions: int = 0    # successful return transitions
        self.revisions: int = 0       # model switches (factor<->tabular)
        self.contract_evals: int = 0  # evaluation windows opened
        self.steps_factor: int = 0
        self.steps_tabular: int = 0

        # rolling error buffer for the expansion test
        self._err_factor: List[float] = []

        self.cost = CogCostCounter()

    def reset(self, env: EnvInstance, rng: np.random.Generator) -> None:
        self.rng = rng
        self.K = env.spec.K
        self.F = env.spec.F

        self.m_hat = np.zeros(self.F, dtype=float)
        self.n_f = np.zeros(self.F, dtype=float)
        self.b_hat = np.zeros(self.F, dtype=float)
        self.n_b = np.zeros(self.F, dtype=float)

        self.q = None
        self.n = None
        self.tabular_active = False

        self.E = 0.0
        self.model = "factor"

        self.expansions = 0
        self.contractions = 0
        self.revisions = 0
        self.contract_evals = 0
        self.steps_factor = 0
        self.steps_tabular = 0

        self._err_factor.clear()
        self.contraction_state = ContractionState()

        # memory scales with F (compressed)
        self.cost = CogCostCounter(mem_units=4.0 * self.F + 1.0, compute_units=0.0, switches=0)

    def _eps(self, t: int) -> float:
        return min(1.0, self.eps0 / math.sqrt(max(1, t)))

    def _maybe_allocate_tabular(self) -> None:
        if self.tabular_active:
            return
        self.q = np.zeros(self.K, dtype=float)
        self.n = np.zeros(self.K, dtype=float)
        self.tabular_active = True
        self.cost.mem_units += 2.0 * self.K
        self.cost.switches += 1
        self.expansions += 1

    def _maybe_free_tabular(self) -> None:
        if not self.tabular_active:
            return
        if not self.contraction_cfg.free_tabular_on_contract:
            return
        # Return memory footprint to the compressed regime.
        self.q = None
        self.n = None
        self.tabular_active = False
        self.cost.mem_units -= 2.0 * self.K

    def _start_contraction_eval(self, env: EnvInstance) -> None:
        """Open an evaluation window in which we compare tabular vs shadow-factor error."""
        if not self.tabular_active:
            return
        assert self.q is not None and self.n is not None

        # Initialize shadow factor model.
        shadow_m = np.zeros(self.F, dtype=float)
        shadow_w = np.zeros(self.F, dtype=float)

        if self.contraction_cfg.init_from_tabular:
            # Weighted group means using visit counts as confidence weights.
            for i in range(self.K):
                f = int(env.c[i])
                w = float(self.n[i])
                if w <= 0.0:
                    continue
                shadow_m[f] += w * float(self.q[i])
                shadow_w[f] += w
            for f in range(self.F):
                if shadow_w[f] > 0.0:
                    shadow_m[f] /= shadow_w[f]
                else:
                    shadow_m[f] = 0.5  # neutral prior if unseen
            shadow_n = shadow_w.copy()
        else:
            shadow_m[:] = 0.5
            shadow_n = np.zeros(self.F, dtype=float)

        self.contraction_state.shadow_m = shadow_m
        self.contraction_state.shadow_n = shadow_n
        self.contraction_state.start(self.contraction_cfg.window)
        self.contract_evals += 1

        # Starting the evaluation window has a meta-cost (building the shadow model).
        self.cost.compute_units += float(self.K)

    def _maybe_expand(self, mse_factor: float) -> None:
        """Expand to tabular when the predictive test flags a mismatch."""
        if not (self.allow_test and self.allow_update):
            return
        if self.model != "factor":
            return
        if len(self._err_factor) < self.window:
            return
        if mse_factor <= self.expand_mse_threshold:
            return

        self._maybe_allocate_tabular()
        if self.allow_update:
            self.model = "tabular"
            self.revisions += 1
            self.cost.switches += 1
            # reset contraction timers once expanded
            self.contraction_state.steps_since_last_eval = 0
            self.contraction_state.stop()

    def _maybe_contract(self) -> None:
        """End-of-window contraction decision."""
        st = self.contraction_state
        if not st.in_eval_window:
            return
        if st.eval_steps_remaining > 0:
            return
        if len(st.err_tab) == 0 or len(st.err_shadow) == 0:
            st.stop()
            return

        # Skip warmup errors: the shadow model learns during the eval window,
        # so early predictions reflect initialization quality, not model fit.
        w = self.contraction_cfg.eval_warmup
        err_tab = st.err_tab[w:]
        err_shadow = st.err_shadow[w:]
        if len(err_tab) == 0 or len(err_shadow) == 0:
            st.stop()
            return

        mse_tab = float(np.mean(err_tab))
        mse_sh = float(np.mean(err_shadow))

        do_contract = (
            self.allow_update
            and self.allow_contract
            and self.contraction_cfg.enabled
            and (mse_sh < (1.0 - self.contraction_cfg.margin) * mse_tab)
        )

        if do_contract:
            assert st.shadow_m is not None and st.shadow_n is not None
            assert self.m_hat is not None and self.n_f is not None

            # Adopt the shadow factor reward estimates.
            self.m_hat[:] = st.shadow_m
            self.n_f[:] = np.maximum(st.shadow_n, 1.0)

            self.model = "factor"
            self.revisions += 1
            self.contractions += 1
            self.cost.switches += 1
            self._maybe_free_tabular()

        st.stop()
        st.steps_since_last_eval = 0

    def act(self, t: int, env: EnvInstance) -> int:
        assert self.rng is not None and self.m_hat is not None and self.b_hat is not None

        # exploration — elevated during contraction eval windows to break
        # on-policy bias (the tabular model is evaluated on actions it selects,
        # so the shadow factor model never gets to demonstrate its advantage
        # on unvisited options without forced exploration)
        eps = self._eps(t)
        if (self.contraction_state.in_eval_window
                and self.contraction_cfg.eval_explore_eps > 0):
            eps = max(eps, self.contraction_cfg.eval_explore_eps)
        if self.rng.random() < eps:
            self.cost.compute_units += 1.0
            return int(self.rng.integers(0, self.K))

        stakes = env.spec.stakes

        if self.model == "factor":
            # factor scores incorporate Variable compression via b_hat and E
            # cost: scan F factors
            self.cost.compute_units += float(self.F)

            pen_f = stakes * (2.0 * self.E * self.b_hat + self.b_hat ** 2)
            score_f = self.m_hat - pen_f
            best_f = int(np.argmax(score_f))

            idx = np.where(env.c == best_f)[0]
            if idx.size == 0:
                self.cost.compute_units += 1.0
                return int(self.rng.integers(0, self.K))

            # local search within factor (bounded, does not scale with K)
            # Smart search: when hedging matters (stakes > 0 and |E| > threshold),
            # prioritize candidates by exposure direction rather than random sampling.
            # This reflects Network's structural understanding: "I know what I'm looking for."
            L = min(self.local_L, idx.size)

            if stakes > 0 and abs(self.E) > 0.5:
                # Directed search based on exposure state
                # E > 0: prefer low b (negative exposure = hedge)
                # E < 0: prefer high b (positive exposure = rebalance toward zero)
                sorted_idx = idx[np.argsort(env.b[idx])]  # ascending by b
                if self.E > 0:
                    cand = sorted_idx[:L]  # lowest b values (hedges)
                else:
                    cand = sorted_idx[-L:]  # highest b values (rebalance)
            else:
                # Random sampling when hedging doesn't matter
                cand = self.rng.choice(idx, size=L, replace=False)

            pen_inc = stakes * (2.0 * self.E * env.b[cand] + env.b[cand] ** 2)
            util_pred = self.m_hat[best_f] - pen_inc
            self.cost.compute_units += float(L)
            return int(cand[int(np.argmax(util_pred))])

        # tabular model (expanded)
        self._maybe_allocate_tabular()
        assert self.q is not None
        self.cost.compute_units += float(self.K)

        pen_inc = stakes * (2.0 * self.E * env.b + env.b ** 2)
        score = self.q - pen_inc
        return int(np.argmax(score))

    def observe(self, a: int, r: float, pen: float, env: EnvInstance) -> None:
        assert self.m_hat is not None and self.n_f is not None and self.b_hat is not None and self.n_b is not None

        # Track time spent in each representation (use the pre-update model).
        if self.model == "factor":
            self.steps_factor += 1
        else:
            self.steps_tabular += 1

        # Update exposure state (always tracked).
        self.E += float(env.b[a])

        f = int(env.c[a])

        # Always update factor reward estimate (regardless of active model).
        # The factor representation is the compressed model; keeping it current
        # ensures it remains a valid contraction target and reflects the
        # structural property that factor learning converges in O(F) observations.
        self.n_f[f] += 1.0
        self.m_hat[f] += (r - self.m_hat[f]) / self.n_f[f]

        # Update factor exposure estimate (b is observed).
        self.n_b[f] += 1.0
        self.b_hat[f] += (float(env.b[a]) - self.b_hat[f]) / self.n_b[f]

        # -------------------------
        # Active-model learning + tests
        # -------------------------

        if self.model == "factor":
            # Rolling predictive error for expansion test.
            pred_f = float(self.m_hat[f])
            self._err_factor.append((r - pred_f) ** 2)
            if len(self._err_factor) > self.window:
                self._err_factor.pop(0)

            if self.allow_test and len(self._err_factor) >= self.window:
                mse_f = float(np.mean(self._err_factor))
                self._maybe_expand(mse_f)

        else:
            # Tabular learning.
            self._maybe_allocate_tabular()
            assert self.q is not None and self.n is not None

            # Capture pre-update prediction for contraction eval BEFORE updating Q.
            # Without this, (r - q[a])² uses the post-update value, giving
            # artificially near-zero error and preventing contraction.
            pred_t_before = float(self.q[a])

            self.n[a] += 1.0
            self.q[a] += (r - self.q[a]) / self.n[a]

            st = self.contraction_state

            # Manage evaluation window scheduling.
            if (not st.in_eval_window) and self.allow_contract and self.allow_update and self.contraction_cfg.enabled:
                st.steps_since_last_eval += 1
                if st.steps_since_last_eval >= self.contraction_cfg.holdout_steps:
                    self._start_contraction_eval(env)
                    st.steps_since_last_eval = 0

            # During the evaluation window, update the shadow model and collect errors.
            if st.in_eval_window and st.shadow_m is not None and st.shadow_n is not None:
                pred_t = pred_t_before
                pred_s = float(st.shadow_m[f])

                st.err_tab.append((r - pred_t) ** 2)
                st.err_shadow.append((r - pred_s) ** 2)

                # Update shadow factor estimate.
                st.shadow_n[f] += 1.0
                st.shadow_m[f] += (r - st.shadow_m[f]) / st.shadow_n[f]

                st.eval_steps_remaining -= 1
                if st.eval_steps_remaining <= 0:
                    st.eval_steps_remaining = 0
                    self._maybe_contract()

        self.cost.compute_units += 10.0

    def cog_cost(self) -> float:
        return self.cost.total(self.cost_w)


# -------------------------
# Experiment runner
# -------------------------

@dataclass(frozen=True)
class RunSpec:
    preset: Literal["smoke", "paper", "boundary-kf", "stakes-sweep", "recovery"]
    seeds: int
    K_values: List[int]
    F: int
    T: int
    stakes_values: List[float]
    structures: List[Structure]
    dual_purpose: bool
    eps0: float
    window: int
    margin: float
    reward_sigma: float
    idio_mu_sigma: float
    idio_b_sigma: float
    factor_corr: float
    cog_lambda: float
    cost_w: CogCostWeights
    regime_change_step: int = 0
    contraction_cfg: ContractionConfig | None = None  # per-preset override; None = agent default


def preset_config(preset: str) -> RunSpec:
    if preset == "smoke":
        return RunSpec(
            preset="smoke",
            seeds=15,
            K_values=[20, 50],
            F=5,
            T=60,
            stakes_values=[0.0, 1.0],
            structures=["structured", "unstructured"],
            dual_purpose=True,
            eps0=0.4,
            window=20,
            margin=0.1,
            reward_sigma=0.25,
            idio_mu_sigma=0.05,
            idio_b_sigma=0.05,
            factor_corr=0.35,
            cog_lambda=1.0,
            cost_w=CogCostWeights(mem=0.01, compute=0.001, switch=25.0),
        )
    if preset == "paper":
        return RunSpec(
            preset="paper",
            seeds=100,
            K_values=[20, 50, 100, 200],
            F=5,
            T=80,
            stakes_values=[0.0, 1.0],
            structures=["structured", "unstructured"],
            dual_purpose=True,
            eps0=0.4,
            window=20,
            margin=0.1,
            reward_sigma=0.25,
            idio_mu_sigma=0.05,
            idio_b_sigma=0.05,
            factor_corr=0.35,
            cog_lambda=1.0,
            cost_w=CogCostWeights(mem=0.01, compute=0.001, switch=25.0),
        )
    # -------------------------------------------------------------------------
    # BOUNDARY CONDITION PRESETS (for reviewer response)
    # -------------------------------------------------------------------------
    if preset == "boundary-kf":
        # Test K=F boundary condition where compression advantage should vanish
        # At K=F, Network has no advantage (nothing to compress)
        return RunSpec(
            preset="boundary-kf",
            seeds=50,
            K_values=[5, 10, 20, 50],  # K=5=F is the critical boundary
            F=5,
            T=80,
            stakes_values=[0.0, 1.0],
            structures=["structured", "unstructured"],
            dual_purpose=True,
            eps0=0.4,
            window=20,
            margin=0.1,
            reward_sigma=0.25,
            idio_mu_sigma=0.05,
            idio_b_sigma=0.05,
            factor_corr=0.35,
            cog_lambda=1.0,
            cost_w=CogCostWeights(mem=0.01, compute=0.001, switch=25.0),
        )
    if preset == "stakes-sweep":
        # Test intermediate stakes values (reviewer point: only tested 0 and 1)
        return RunSpec(
            preset="stakes-sweep",
            seeds=50,
            K_values=[50, 200],  # Two representative K values
            F=5,
            T=80,
            stakes_values=[0.0, 0.25, 0.5, 0.75, 1.0],  # Full sweep
            structures=["structured"],  # Focus on structured for stakes effect
            dual_purpose=True,
            eps0=0.4,
            window=20,
            margin=0.1,
            reward_sigma=0.25,
            idio_mu_sigma=0.05,
            idio_b_sigma=0.05,
            factor_corr=0.35,
            cog_lambda=1.0,
            cost_w=CogCostWeights(mem=0.01, compute=0.001, switch=25.0),
        )
    if preset == "recovery":
        # Regime-change test for Proposition 1(iii): bounded recovery.
        # Starts unstructured (factor hypothesis wrong → expansion to tabular),
        # then switches to structured at step 100 (factor hypothesis now correct →
        # contraction should fire within holdout + window steps).
        # T=300 gives 200 post-change steps, well above holdout(50)+window(20)=70.
        return RunSpec(
            preset="recovery",
            seeds=100,
            K_values=[50, 200],
            F=5,
            T=300,
            stakes_values=[0.0, 1.0],
            structures=["recovery"],
            dual_purpose=True,
            eps0=0.4,
            window=20,
            margin=0.1,
            reward_sigma=0.25,
            idio_mu_sigma=0.05,
            idio_b_sigma=0.05,
            factor_corr=0.35,
            cog_lambda=1.0,
            cost_w=CogCostWeights(mem=0.01, compute=0.001, switch=25.0),
            regime_change_step=100,
            contraction_cfg=ContractionConfig(
                enabled=True,
                holdout_steps=120,   # max(50, 3*40) = 120
                window=40,           # wider window: more signal for contraction decision
                margin=0.05,         # lower threshold: contract if MSE_shadow < 0.95 * MSE_tab
                init_from_tabular=False,  # start shadow from fresh prior (avoids contamination from pre-change data)
                free_tabular_on_contract=True,
                eval_explore_eps=0.2,  # force 20% exploration during eval to break on-policy bias
                eval_warmup=20,      # skip first 20 steps of eval (shadow model learning warmup)
            ),
        )
    raise ValueError(f"Unknown preset: {preset}")


def run_episode(env: EnvInstance, agent: Agent, rng: np.random.Generator) -> Dict[str, float]:
    # Save original mu so regime-change mutation doesn't leak across agents
    mu_original = env.mu.copy() if env.mu_alt is not None else None

    agent.reset(env, rng)
    E = 0.0
    total_r = 0.0
    total_pen = 0.0
    dual_picks = 0
    dual_picks_cond = 0  # conditional: only when |E| >= 0.5
    dual_cond_steps = 0   # steps where |E| >= 0.5
    E_abs_sum = 0.0

    for t in range(1, env.spec.T + 1):
        # Regime change: swap reward means when crossing the boundary
        if (env.spec.regime_change_step > 0
                and t == env.spec.regime_change_step
                and env.mu_alt is not None):
            env.mu = env.mu_alt.copy()

        a = agent.act(t, env)
        r = step_reward(rng, env.mu[a], env.spec.reward_sigma)
        pen = step_penalty(E, env.b[a], env.spec.stakes)

        # Track conditional dual picks (when hedging matters)
        if abs(E) >= 0.5:
            dual_cond_steps += 1
            if env.dual_mask[a]:
                dual_picks_cond += 1

        E += float(env.b[a])
        E_abs_sum += abs(E)

        total_r += r
        total_pen += pen
        if env.dual_mask[a]:
            dual_picks += 1

        agent.observe(a, r, pen, env)

    util = total_r - total_pen
    cog = agent.cog_cost()

    # Get revision-cycle metrics from Network agents (other agents default to 0)
    expansions = float(getattr(agent, "expansions", 0))
    contractions = float(getattr(agent, "contractions", 0))
    revisions = float(getattr(agent, "revisions", 0))
    contract_evals = float(getattr(agent, "contract_evals", 0))
    steps_factor = float(getattr(agent, "steps_factor", 0))
    steps_tabular = float(getattr(agent, "steps_tabular", 0))

    result = dict(
        total_reward=total_r,
        total_penalty=total_pen,
        utility=util,
        cognitive_cost=cog,
        dual_picks=float(dual_picks),
        dual_picks_cond=float(dual_picks_cond),
        dual_cond_steps=float(dual_cond_steps),
        E_abs_mean=E_abs_sum / float(env.spec.T),
        expansions=expansions,
        contractions=contractions,
        revisions=revisions,
        contract_evals=contract_evals,
        steps_factor=steps_factor,
        steps_tabular=steps_tabular,
    )

    # Restore original mu so env can be reused across agents
    if mu_original is not None:
        env.mu = mu_original

    return result


def run_suite(cfg: RunSpec, out_dir: str) -> pd.DataFrame:
    out = Path(out_dir)
    results_dir = out / "results"
    artifacts_dir = out / "artifacts"
    (artifacts_dir / "tables").mkdir(parents=True, exist_ok=True)
    (artifacts_dir / "figures").mkdir(parents=True, exist_ok=True)
    results_dir.mkdir(parents=True, exist_ok=True)

    rows: List[Dict[str, object]] = []

    # Build agents list once to determine count for RNG spawning.
    # IMPORTANT: new agents are appended AFTER existing agents to preserve
    # RNG streams for the original 7 agents (indices 0-6).
    _sample_agents: List[Agent] = [
        DotRandom(cfg.cost_w),
        LinearTabular(cfg.eps0, cfg.cost_w),
        ThompsonFactor(cfg.cost_w, sigma_r=cfg.reward_sigma),
        NetworkCycle(cfg.eps0, cfg.window, cfg.margin, True, True, cfg.cost_w, contraction=cfg.contraction_cfg),
        NetworkCycle(cfg.eps0, cfg.window, cfg.margin, True, True, cfg.cost_w, allow_contract=False),
        NetworkCycle(cfg.eps0, cfg.window, cfg.margin, False, True, cfg.cost_w),
        NetworkCycle(cfg.eps0, cfg.window, cfg.margin, True, False, cfg.cost_w),
        # --- New baseline agents (indices 7-10) ---
        UCB1Agent(cfg.cost_w),
        HierBayes(cfg.cost_w, sigma_r=cfg.reward_sigma),
        RWFeatureAgent(cfg.eps0, cfg.cost_w),
        LatentFactorEM(cfg.cost_w, sigma_r=cfg.reward_sigma),
    ]
    n_agents = len(_sample_agents)

    for seed in range(cfg.seeds):
        # Separate RNG streams: one for environment, one per agent.
        # Per-agent streams ensure adding/removing agents doesn't shift
        # results for the others (no RNG coupling).
        env_rng, agent_rngs = spawn_rngs(seed, n_agents)
        for K in cfg.K_values:
            for stakes in cfg.stakes_values:
                for structure in cfg.structures:
                    spec = EnvSpec(
                        K=K,
                        F=cfg.F,
                        T=cfg.T,
                        stakes=stakes,
                        reward_sigma=cfg.reward_sigma,
                        idio_mu_sigma=cfg.idio_mu_sigma,
                        idio_b_sigma=cfg.idio_b_sigma,
                        factor_corr=cfg.factor_corr,
                        structure=structure,
                        regime_change_step=cfg.regime_change_step,
                        dual_purpose=cfg.dual_purpose,
                    )
                    env = make_env(env_rng, spec)

                    agents: List[Agent] = [
                        DotRandom(cfg.cost_w),
                        LinearTabular(cfg.eps0, cfg.cost_w),
                        ThompsonFactor(cfg.cost_w, sigma_r=cfg.reward_sigma),
                        NetworkCycle(cfg.eps0, cfg.window, cfg.margin, True, True, cfg.cost_w, contraction=cfg.contraction_cfg),
                        NetworkCycle(cfg.eps0, cfg.window, cfg.margin, True, True, cfg.cost_w, allow_contract=False),
                        NetworkCycle(cfg.eps0, cfg.window, cfg.margin, False, True, cfg.cost_w),
                        NetworkCycle(cfg.eps0, cfg.window, cfg.margin, True, False, cfg.cost_w),
                        # --- New baseline agents (indices 7-10) ---
                        UCB1Agent(cfg.cost_w),
                        HierBayes(cfg.cost_w, sigma_r=cfg.reward_sigma),
                        RWFeatureAgent(cfg.eps0, cfg.cost_w),
                        LatentFactorEM(cfg.cost_w, sigma_r=cfg.reward_sigma),
                    ]

                    for i, agent in enumerate(agents):
                        ep = run_episode(env, agent, agent_rngs[i])
                        cau = float(ep["utility"]) - cfg.cog_lambda * float(ep["cognitive_cost"])
                        # Compute conditional dual rate (only when hedging matters: |E| >= 0.5)
                        dual_rate = float(ep["dual_picks"]) / float(cfg.T)
                        dual_cond_steps = float(ep["dual_cond_steps"])
                        dual_rate_cond = float(ep["dual_picks_cond"]) / max(1.0, dual_cond_steps)
                        rows.append(
                            dict(
                                seed=seed,
                                agent=agent.name,
                                K=K,
                                F=cfg.F,
                                T=cfg.T,
                                stakes=stakes,
                                structure=structure,
                                total_reward=float(ep["total_reward"]),
                                total_penalty=float(ep["total_penalty"]),
                                utility=float(ep["utility"]),
                                cognitive_cost=float(ep["cognitive_cost"]),
                                cost_adjusted_utility=float(cau),
                                dual_picks=float(ep["dual_picks"]),
                                dual_rate=dual_rate,
                                dual_rate_cond=dual_rate_cond,
                                E_abs_mean=float(ep["E_abs_mean"]),
                                expansions=float(ep["expansions"]),
                                contractions=float(ep["contractions"]),
                                revisions=float(ep["revisions"]),
                                contract_evals=float(ep["contract_evals"]),
                                steps_factor=float(ep["steps_factor"]),
                                steps_tabular=float(ep["steps_tabular"]),
                            )
                        )

    df = pd.DataFrame(rows)
    df_path = results_dir / "episode_metrics.csv"
    df.to_csv(df_path, index=False)

    # Aggregation
    agg = (
        df.groupby(["agent", "structure", "stakes", "K"], as_index=False)
        .agg(
            reward_mean=("total_reward", "mean"),
            reward_std=("total_reward", "std"),
            penalty_mean=("total_penalty", "mean"),
            penalty_std=("total_penalty", "std"),
            utility_mean=("utility", "mean"),
            utility_std=("utility", "std"),
            cog_mean=("cognitive_cost", "mean"),
            cog_std=("cognitive_cost", "std"),
            cau_mean=("cost_adjusted_utility", "mean"),
            cau_std=("cost_adjusted_utility", "std"),
            dual_mean=("dual_picks", "mean"),
            dual_rate_mean=("dual_rate", "mean"),
            dual_rate_std=("dual_rate", "std"),
            dual_rate_cond_mean=("dual_rate_cond", "mean"),
            dual_rate_cond_std=("dual_rate_cond", "std"),
            E_abs_mean=("E_abs_mean", "mean"),
            expansions_mean=("expansions", "mean"),
            contractions_mean=("contractions", "mean"),
            revisions_mean=("revisions", "mean"),
            contract_evals_mean=("contract_evals", "mean"),
            steps_factor_mean=("steps_factor", "mean"),
            steps_tabular_mean=("steps_tabular", "mean"),
        )
    )
    table_path = artifacts_dir / "tables" / "agg_summary.csv"
    agg.to_csv(table_path, index=False)

    # Figures
    import matplotlib
    matplotlib.use("Agg")
    import matplotlib.pyplot as plt

    # --- DLN stage visual grouping ---
    # Same color family = same DLN stage; different line style = different algorithm
    _agent_style: Dict[str, Tuple[str, str, str]] = {
        # (color, linestyle, marker)
        # Dot stage (gray)
        "Dot":               ("#888888", "-",  "o"),
        # Linear stage (blue family)
        "Linear":            ("#1f77b4", "-",  "o"),
        "UCB1":              ("#1f77b4", "--", "s"),
        # Linear-Plus stage (green family)
        "Thompson-Factor":   ("#2ca02c", "-",  "o"),
        "RW-Feature":        ("#2ca02c", "--", "s"),
        # Network standard (orange family)
        "HierBayes":         ("#ff7f0e", "-",  "o"),
        "LatentFactorEM":    ("#ff7f0e", "--", "s"),
        # Network DLN (red/purple family)
        "Network-Full":      ("#d62728", "-",  "o"),
        "Network-NoContract":("#d62728", "--", "s"),
        "Network-NoTest":    ("#9467bd", "-",  "^"),
        "Network-NoUpdate":  ("#9467bd", "--", "D"),
    }

    def _plot_agent(ax_or_plt, agent_name: str, x, y, yerr=None, **kw):
        """Plot a single agent line with consistent DLN-stage styling."""
        color, ls, marker = _agent_style.get(agent_name, ("#333333", "-", "o"))
        if yerr is not None:
            ax_or_plt.errorbar(x, y, yerr=yerr, color=color, linestyle=ls,
                               marker=marker, label=agent_name, capsize=3, **kw)
        else:
            ax_or_plt.plot(x, y, color=color, linestyle=ls, marker=marker,
                           label=agent_name, **kw)

    # Fig 1a: stable structured env, stakes=0 — C1 compression story only
    # Thompson-Factor outperforms Network-Full here (better exploration, not
    # topology) which confuses the C1 narrative. Keep it for stakes=1 only.
    _fig1_original = ["Dot", "Linear",
                      "Network-Full", "Network-NoContract", "Network-NoTest", "Network-NoUpdate"]
    plt.figure(figsize=(10, 6))
    sub = agg[(agg["structure"] == "structured") & (agg["stakes"] == 0.0)]
    for agent in _fig1_original:
        s = sub[sub["agent"] == agent].sort_values("K")
        if len(s) == 0:
            continue
        _plot_agent(plt, agent, s["K"], s["cau_mean"], yerr=s["cau_std"])
    plt.xlabel("Number of options K")
    plt.ylabel("Cost-adjusted utility (mean ± sd)")
    plt.title("Stable environment (structured), stakes=0")
    plt.legend(fontsize=7, ncol=2)
    plt.tight_layout()
    plt.savefig(artifacts_dir / "figures" / "fig_cau_vs_K_structured_stakes0.0.png", dpi=200)
    plt.close()

    # Fig 1b: stable structured env, stakes=1 — all agents, symlog scale
    # New agents shown by DLN stage; symlog makes Network-Full's positive
    # CAU visible alongside the large negative collapses.
    # Legend ordered by DLN stage: Dot → Linear → Linear-Plus → Network std → Network DLN
    _fig1_stakes1 = [
        # Dot
        "Dot",
        # Linear stage
        "Linear", "UCB1",
        # Linear-Plus stage
        "Thompson-Factor", "RW-Feature",
        # Network standard (no DLN mechanisms)
        "HierBayes", "LatentFactorEM",
        # Network DLN (full mechanisms)
        "Network-Full", "Network-NoContract", "Network-NoTest", "Network-NoUpdate",
    ]
    plt.figure(figsize=(10, 6))
    sub = agg[(agg["structure"] == "structured") & (agg["stakes"] == 1.0)]
    for agent in _fig1_stakes1:
        s = sub[sub["agent"] == agent].sort_values("K")
        if len(s) == 0:
            continue
        _plot_agent(plt, agent, s["K"], s["cau_mean"], yerr=s["cau_std"])
    plt.xlabel("Number of options K")
    plt.ylabel("Cost-adjusted utility (mean ± sd, symlog scale)")
    plt.title("Stable environment (structured), stakes=1 — by DLN stage")
    plt.yscale("symlog", linthresh=100)
    plt.legend(fontsize=7, ncol=2)
    plt.tight_layout()
    plt.savefig(artifacts_dir / "figures" / "fig_cau_vs_K_structured_stakes1.0.png", dpi=200)
    plt.close()

    # Fig 2: CONDITIONAL dual-purpose pick rate (when hedging matters: |E| >= 0.5)
    # Legend ordered by DLN stage for consistency with stakes=1 figure.
    plt.figure(figsize=(10, 6))
    sub = agg[(agg["structure"] == "structured") & (agg["stakes"] == 1.0)]
    for agent in ["Linear", "UCB1",
                   "Thompson-Factor", "RW-Feature",
                   "HierBayes", "LatentFactorEM",
                   "Network-Full", "Network-NoContract", "Network-NoTest", "Network-NoUpdate"]:
        s = sub[sub["agent"] == agent].sort_values("K")
        if len(s) == 0:
            continue
        _plot_agent(plt, agent, s["K"], s["dual_rate_cond_mean"], yerr=s["dual_rate_cond_std"])
    plt.xlabel("Number of options K")
    plt.ylabel("Conditional dual-purpose pick rate")
    plt.title("Dual-purpose discovery when hedging matters (|E| ≥ 0.5)")
    plt.legend(fontsize=7, ncol=2)
    plt.tight_layout()
    plt.savefig(artifacts_dir / "figures" / "fig_dual_purpose_rate.png", dpi=200)
    plt.close()

    # Fig 3: cycle response in unstructured, stakes=0 (prior-wrong) - show expansions
    # Only Network variants can expand; other agents have 0 expansions by design.
    plt.figure()
    sub = agg[(agg["structure"] == "unstructured") & (agg["stakes"] == 0.0)]
    for agent in ["Network-Full", "Network-NoContract", "Network-NoTest", "Network-NoUpdate"]:
        s = sub[sub["agent"] == agent].sort_values("K")
        if len(s) == 0:
            continue
        _plot_agent(plt, agent, s["K"], s["expansions_mean"])
    plt.xlabel("Number of options K")
    plt.ylabel("Mean hypothesis expansions")
    plt.title("Prior-wrong condition (unstructured): cycle response")
    plt.legend()
    plt.tight_layout()
    plt.savefig(artifacts_dir / "figures" / "fig_switches.png", dpi=200)
    plt.close()

    # Fig 4: recovery contraction metrics (only generated when recovery data exists)
    rec = agg[agg["structure"] == "recovery"]
    if len(rec) > 0:
        fig = plt.figure(figsize=(14, 10))
        gs = fig.add_gridspec(2, 2, height_ratios=[1, 1], hspace=0.35, wspace=0.30)

        # Panel A (top-left): contraction rate vs K by stakes
        ax = fig.add_subplot(gs[0, 0])
        for stakes in sorted(rec["stakes"].unique()):
            net = rec[(rec["agent"] == "Network-Full") & (rec["stakes"] == stakes)].sort_values("K")
            if len(net) > 0:
                ax.plot(net["K"], net["contractions_mean"], marker="o",
                        label=f"stakes={stakes}")
        ax.set_xlabel("Number of options K")
        ax.set_ylabel("Contraction rate (fraction of episodes)")
        ax.set_title("Return transitions (G_tab → G_F)")
        ax.axhline(y=0.30, color="gray", linestyle="--", alpha=0.5, label="30% target")
        ax.legend()

        # Panel B (top-right): regime-change CAU with stakes (all agents, ordered by DLN stage)
        ax = fig.add_subplot(gs[0, 1])
        stakes_sub = rec[rec["stakes"] == 1.0]
        for agent in ["Dot",
                       "Linear", "UCB1",
                       "Thompson-Factor", "RW-Feature",
                       "HierBayes", "LatentFactorEM",
                       "Network-Full", "Network-NoContract"]:
            s = stakes_sub[stakes_sub["agent"] == agent].sort_values("K")
            if len(s) > 0:
                _plot_agent(ax, agent, s["K"], s["cau_mean"])
        ax.set_xlabel("Number of options K")
        ax.set_ylabel("Cost-adjusted utility")
        ax.set_title("Regime change + stakes (λ=1) — by DLN stage")
        ax.set_yscale("symlog", linthresh=100)
        ax.legend(fontsize=7, ncol=2)

        # Panel C (bottom, spanning full width): time allocation
        ax = fig.add_subplot(gs[1, :])
        net = rec[(rec["agent"] == "Network-Full")].sort_values(["stakes", "K"])
        for stakes in sorted(rec["stakes"].unique()):
            s = net[net["stakes"] == stakes].sort_values("K")
            if len(s) > 0:
                ax.bar(s["K"].astype(str) + f"\nλ={stakes}",
                       s["steps_factor_mean"], label="Factor model" if stakes == 0.0 else "",
                       color="steelblue", alpha=0.7)
                ax.bar(s["K"].astype(str) + f"\nλ={stakes}",
                       s["steps_tabular_mean"], bottom=s["steps_factor_mean"],
                       label="Tabular model" if stakes == 0.0 else "",
                       color="salmon", alpha=0.7)
        ax.set_ylabel("Mean steps per episode")
        ax.set_title("Time in each model class")
        ax.legend()

        fig.suptitle("Recovery preset: contraction mechanism evaluation", fontsize=13, y=0.98)
        fig.savefig(artifacts_dir / "figures" / "fig_contraction_recovery.png", dpi=200)
        plt.close(fig)

    manifest = dict(
        preset=cfg.preset,
        seeds=cfg.seeds,
        K_values=cfg.K_values,
        F=cfg.F,
        T=cfg.T,
        stakes_values=cfg.stakes_values,
        structures=cfg.structures,
        dual_purpose=cfg.dual_purpose,
        eps0=cfg.eps0,
        window=cfg.window,
        margin=cfg.margin,
        reward_sigma=cfg.reward_sigma,
        idio_mu_sigma=cfg.idio_mu_sigma,
        idio_b_sigma=cfg.idio_b_sigma,
        factor_corr=cfg.factor_corr,
        cog_lambda=cfg.cog_lambda,
        cost_weights=dataclasses.asdict(cfg.cost_w),
        outputs=dict(
            episode_metrics=str(df_path),
            agg_summary=str(table_path),
            figures_dir=str(artifacts_dir / "figures"),
        ),
    )
    (results_dir / "manifest.json").write_text(json.dumps(manifest, indent=2))

    return df


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--preset", choices=["smoke", "paper", "boundary-kf", "stakes-sweep", "recovery"], default="paper")
    ap.add_argument("--out", default="./dln_outputs")
    args = ap.parse_args()

    cfg = preset_config(args.preset)
    run_suite(cfg, args.out)

    print("DLN simulation complete.")
    print(f"Outputs written to: {args.out}")


if __name__ == "__main__":
    main()
